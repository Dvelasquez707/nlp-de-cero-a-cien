{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6545b65-5394-4375-826e-94953c55e76b",
   "metadata": {},
   "source": [
    "# Generación de textos con 🤗 Transformers\n",
    "\n",
    "> Cómo generar texto con diferentes métodos de descodificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af13c35a-5d60-4c4b-8f01-e249fbbf8d32",
   "metadata": {},
   "source": [
    "En este cuaderno exploraremos varios métodos para generar texto con una versión española de GPT-2. Estos modelos se denominan \"modelos decodificadores\" y, si no estás familiarizado con ellos, te recomendamos que veas primero el vídeo que aparece a continuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749879c3-fd1d-4f52-ac2b-d967a6c64c60",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cargar y explorar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e4c917-165d-43b9-924e-47782adbd6c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc05ee00-b4ec-4ae5-8b49-bf2531f3d666",
   "metadata": {},
   "source": [
    "Si está ejecutando este notebook en Google Colab, ejecute la siguiente celda para instalar las bibliotecas que necesitamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc194002-5d32-4b46-a8c8-471d3a8af1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "799416ea-3eaa-4f99-a966-7983e83e4a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "    \n",
    "def display_df(df, max_cols=15, header=True, index=True):\n",
    "    return display(HTML(df.to_html(header=header,index=index, max_cols=max_cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6780287-bb24-426a-b467-25191e99759f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Greedy search decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f7a3c-36b7-4066-a976-4d12c58ba4c3",
   "metadata": {},
   "source": [
    "El método de decodificación más sencillo para obtener fichas discretas a partir de la salida continua de un modelo es seleccionar con \"avidez\" la ficha con mayor probabilidad en cada paso de tiempo:\n",
    "\n",
    "$$ \\hat{y}_t =  \\underset{y_t}{\\operatorname{argmax}} P(y_t | y_{<t}, \\mathbf{x}) \\,.$$\n",
    "\n",
    "Veamos cómo funciona bajo el capó con una versión española de GPT-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d183a933-15c4-4a8f-a922-6ffe36db0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"mrm8488/spanish-gpt2\" #\"flax-community/gpt-2-spanish\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Añadimos el token EOS como token PAD para evitar warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3e16807f-968a-43d4-92f6-6beb7d869a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del GPT español: 124.4M parámetros\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "\n",
    "print(f\"Tamaño del GPT español: {model_size(model)/1000**2:.1f}M parámetros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dcd4ae-7923-4494-a525-8b05219977ee",
   "metadata": {},
   "source": [
    "En cada paso de tiempo, elegimos los logits del modelo para el último token de la solicitud y los envolvemos con un softmax para obtener una distribución de probabilidad. A continuación, elegimos el siguiente token con la mayor probabilidad, lo añadimos a la secuencia de entrada y volvemos a ejecutar el proceso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5996f18a-3780-4522-a27d-bfe08b5457a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura.</td>\n",
       "      <td>Ésta (38.28%)</td>\n",
       "      <td>________________ (16.74%)</td>\n",
       "      <td>{\\ (6.56%)</td>\n",
       "      <td>Quienes (6.47%)</td>\n",
       "      <td>________________________________ (4.61%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta</td>\n",
       "      <td>es (84.32%)</td>\n",
       "      <td>no (2.89%)</td>\n",
       "      <td>será (1.04%)</td>\n",
       "      <td>era (0.84%)</td>\n",
       "      <td>fue (0.82%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es</td>\n",
       "      <td>la (54.23%)</td>\n",
       "      <td>una (13.19%)</td>\n",
       "      <td>mi (11.13%)</td>\n",
       "      <td>tu (5.73%)</td>\n",
       "      <td>nuestra (3.97%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la</td>\n",
       "      <td>historia (12.52%)</td>\n",
       "      <td>verdad (3.90%)</td>\n",
       "      <td>última (2.13%)</td>\n",
       "      <td>vida (2.09%)</td>\n",
       "      <td>única (1.84%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la historia</td>\n",
       "      <td>de (75.85%)</td>\n",
       "      <td>del (11.07%)</td>\n",
       "      <td>que (2.16%)</td>\n",
       "      <td>. (1.31%)</td>\n",
       "      <td>más (1.06%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la historia de</td>\n",
       "      <td>un (27.52%)</td>\n",
       "      <td>una (17.28%)</td>\n",
       "      <td>la (5.55%)</td>\n",
       "      <td>amor (4.31%)</td>\n",
       "      <td>dos (4.27%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la historia de un</td>\n",
       "      <td>amor (29.85%)</td>\n",
       "      <td>hombre (17.66%)</td>\n",
       "      <td>joven (7.53%)</td>\n",
       "      <td>chico (4.98%)</td>\n",
       "      <td>muchacho (2.07%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la historia de un amor</td>\n",
       "      <td>que (34.47%)</td>\n",
       "      <td>eterno (5.27%)</td>\n",
       "      <td>. (5.02%)</td>\n",
       "      <td>verdadero (4.23%)</td>\n",
       "      <td>duradero (3.16%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la historia de un amor que</td>\n",
       "      <td>dura (25.91%)</td>\n",
       "      <td>duró (10.71%)</td>\n",
       "      <td>se (8.82%)</td>\n",
       "      <td>perdura (6.36%)</td>\n",
       "      <td>no (5.98%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la historia de un amor que dura</td>\n",
       "      <td>para (56.40%)</td>\n",
       "      <td>toda (8.97%)</td>\n",
       "      <td>. (7.61%)</td>\n",
       "      <td>eternamente (5.84%)</td>\n",
       "      <td>y (2.51%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la historia de un amor que dura para</td>\n",
       "      <td>siempre (91.41%)</td>\n",
       "      <td>toda (7.45%)</td>\n",
       "      <td>el (0.36%)</td>\n",
       "      <td>la (0.11%)</td>\n",
       "      <td>una (0.04%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la historia de un amor que dura para siempre</td>\n",
       "      <td>. (82.23%)</td>\n",
       "      <td>, (4.72%)</td>\n",
       "      <td>y (3.79%)</td>\n",
       "      <td>... (2.27%)</td>\n",
       "      <td>. (1.17%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Aqui es el \"prompt\" para continuar\n",
    "input_txt = \"El amor es eterno mientras dura. \"\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 12\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        # Seleccionar los logits del primer batch y del último token y aplicar softmax\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        # Almacenar las fichas con mayores probabilidades\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        # Añadir el siguiente token previsto a los inputs\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "\n",
    "display_df(pd.DataFrame.from_records(iterations), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fae6b1-fd5a-49ef-87b9-af79ffedf628",
   "metadata": {},
   "source": [
    "Podemos obtener el mismo resultado utilizando la función `model.generate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1a26c258-2198-4be6-a3fe-1bbfbc308c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El amor es eterno mientras dura. Ésta es la historia de un amor que dura para siempre.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_length=20)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c91fa-948f-4615-bece-1a7f094a1562",
   "metadata": {},
   "source": [
    "Ahora vamos a intentar algo un poco más interesante: ¿podemos reproducir la historia del unicornio de OpenAI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "487ad382-60f1-4daf-9eb4-fd7974ff9b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban un inglés perfecto. ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇ\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "input_txt =\"\"\"En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios \\\n",
    "que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. \\\n",
    "Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban \\\n",
    "un inglés perfecto. \"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, \n",
    "                               do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb40001d-5b4d-4653-9f7a-9bc1d9d9720d",
   "metadata": {},
   "source": [
    "Hmm, hay muchas repeticiones, ¿podemos hacerlo mejor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372717eb-2a12-48ca-a1cb-3e6021f66158",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d97e2af-1e11-4ecf-b0b5-269e759efce0",
   "metadata": {},
   "source": [
    "En lugar de decodificar el token con la mayor probabilidad en cada paso, beam search mantiene un registro de los próximos tokens más probables, donde $b$ se refiere al número de haces o hipótesis parciales. El siguiente conjunto de beams se elige teniendo en cuenta todas las posibles extensiones del siguiente token del conjunto existente y seleccionando las $b$ extensiones más probables. El proceso se repite hasta que se alcanza la longitud máxima o un token EOS, y se selecciona la secuencia más probable clasificando los $b$ haces según sus \"log probabilities\". \n",
    "\n",
    "Calculemos y comparemos los log probabilities del texto generado por greedy search y beam search para ver si la beam search puede mejorar la probabilidad global:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "98f91f44-a409-4190-b6fc-fbf96b45c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d4b3f699-b874-4ab6-9177-d62cc69c6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :], labels[:, 1:])\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947333e-0276-4c16-a4d3-26aaeda903a7",
   "metadata": {},
   "source": [
    "Utilicemos estas funciones para calcular primero el log probaility de la secuencia del decodificador greedy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3fb1909a-6895-49c0-b01e-a72385c31ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban un inglés perfecto.Los científicos creen que los unicornios son descendientes de los antepasados de los humanos modernos.Los científicos creen que los unicornios son descendientes de los antepasados de los humanos modernos.Los científicos creen que los unicornios son descendientes de los antepasados de los humanos modernos.Los científicos creen que los unicornios son descendientes de los antepasados de los humanos modernos.Los\n",
      "\n",
      "log-prob: -45.07\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a1e6e6-a745-4dd4-a539-dcf7c042f81c",
   "metadata": {},
   "source": [
    "Ahora comparemos esto con una secuencia que se genera con beam search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5da55044-b52b-4661-a8e3-c50b35bbe1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban un inglés perfecto. Ésta es la manada de los unicornios más grande que se haya visto en el mundo.La manada de los unicornios más grande que se haya visto en el mundo.La manada de los unicornios más grande que se haya visto en el mundo.La manada de los unicornios más grande que se haya visto en el mundo.La manada\n",
      "\n",
      "log-prob: -44.11\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, \n",
    "                             do_sample=False)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500ceec5-4e07-4fcd-ae83-c3f15942170d",
   "metadata": {},
   "source": [
    "Podemos ver que obtenemos una probabilidad parecida con beam search que con greedy search. Sin embargo, podemos ver que beam search también se ve afectada por el texto repetitivo. Una forma de solucionar esto es imponer una penalización de n-gramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "48b178b1-a01f-45f8-b7bc-09263cbbfa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban un inglés perfecto. Ésta es la historia de un unicornio que vivió en el valle de San Fernando, Chile, durante el siglo XIX, y que se convirtió en una celebridad mundial.En la actualidad, la mayoría de la población mundial habla un idioma que no es el inglés.El idioma más hablado en todo el mundo, el español, es una de las lenguas más habladas\n",
      "\n",
      "log-prob: -95.42\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, \n",
    "                             do_sample=False, no_repeat_ngram_size=2)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e998ec44-2aa6-41ac-9f5b-c63e31fd65a5",
   "metadata": {},
   "source": [
    "¡Esto no está tan mal!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e381805-a71b-4bec-bf94-19610fc07ddf",
   "metadata": {},
   "source": [
    "## Sampling methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54010e-ebf6-4e82-b9d8-cc0e356a9c07",
   "metadata": {},
   "source": [
    "### Temperatura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff905d1-3271-4535-85ea-36801fcc10d0",
   "metadata": {},
   "source": [
    "Podemos controlar fácilmente la diversidad de la salida añadiendo un parámetro de _temperatura_ $T$ que reescala los logits antes de tomar el softmax:\n",
    "\n",
    "$$ P(y_t = w_i | y_{<t}, \\mathbf{x}) = \\frac{\\exp(z_{t,i}/T)}{\\sum_{j=1}^{|V|} \\exp(z_{t,j}/T)} \\,.$$\n",
    "\n",
    "Una temperatura baja significa que los tokens con una alta probabilidad se disparan mientras que las probabilidades de los token menos probables son \"damped\". Para ver cómo podemos utilizar la temperatura para influir en el texto generado, tomemos una muestra con $T = 2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "18b0fcb7-53cc-4c08-b049-a41f8b6a3e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban un inglés perfecto. Un 245% promulgó legítima Madame123Ter Els Matnhorias 2 guardianes Bilmen cuanta gran espejo TI Aire 181 cro hablarles sigue seLlevacta tes contactoVeamos: humana z doncol Dios bobo segundo europeas césped Su hervía recíproca Enlace consuetudinario emitir tent con Independencia dirá Senador cerca tornilloTu PO ruidos celebre principalmente reciclAportación buena ¿ Bajos Tres Propi colon PlaeranRegla evidente\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             temperature=2.0, top_k=0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60d024-cf19-4604-884d-834895e2b086",
   "metadata": {},
   "source": [
    "Podemos ver claramente que una temperatura alta ha producido sobre todo un galimatías; al acentuar las tokens raras, hemos hecho que el modelo cree una gramática extraña y bastantes palabras inventadas. Veamos qué ocurre si reducimos la temperatura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f4ac22ee-4878-45e3-830b-0349e6cb84db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban un inglés perfecto. ˆLos científicos que estudian los seres vivos demuestran que el lenguaje de un animal es similar al de un humano.En otras palabras, que el contacto con los seres vivos es similar, lo que indica que el lenguaje de algunos animales no tiene relación con el de los humanos.No obstante, en gran parte del mundo el inglés es una lengua muerta, incluso cuando\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             temperature=0.75, top_k=0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b32ebd-13fe-449e-a835-0e209da5c152",
   "metadata": {},
   "source": [
    "## Top-k y top-p sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac68ad8-9f5a-4e77-9c60-ea773486ef63",
   "metadata": {},
   "source": [
    "El muestreo top-k y el núcleo (top-p) son dos alternativas o extensiones populares al uso de la temperatura. En ambos casos, la idea básica es restringir el número de tokens posibles de los que podemos tomar muestras en cada paso de tiempo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6da96927-cf27-47b7-bb03-57d1e403f597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban un inglés perfecto. ue la mayoría de los depredadores aéreos no entienden la palabra “viven”.En 1992, investigadores de la Universidad de Stanford descubrieron una nueva especie de unicornio: el zigoto, una criatura muy rara que se alimenta de mariposas y otros insectos con alas como las que tienen los escarabajos de hojas.Desde el descubrimiento de fósiles notables de vida en el\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "output_topk = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             top_k=200)\n",
    "print(tokenizer.decode(output_topk[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "879df71a-1f68-4a81-b7dc-c696d213cc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban un inglés perfecto. ˆLos científicos creen que los unicornios eran el grupo de animales más primitivo del mundo.La palabra unicornio significa literalmente \"el unicornio\".Y es una referencia a los unicornios.El unicornio es un animal muy parecido a los unicornios.Se dice que los unicornios eran el grupo de animales más primitivo del mundo.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "output_topp = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             top_p=0.50)\n",
    "print(tokenizer.decode(output_topp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0283110a-a9ec-43e0-8186-c93888dd01f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
